{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set 3: Text analysis of DOJ press releases\n",
    "\n",
    "**Total points (without extra credit)**: 52 \n",
    "\n",
    "- For background:\n",
    "\n",
    "    - DOJ is the federal law enforcement agency responsible for federal prosecutions; this contrasts with the local prosecutions in the Cook County dataset we analyzed earlier. Here's a short explainer on which crimes get prosecuted federally versus locally: https://www.criminaldefenselawyer.com/resources/criminal-defense/federal-crime/state-vs-federal-crimes.htm#:~:text=Federal%20criminal%20prosecutions%20are%20handled,of%20state%20and%20local%20law. \n",
    "    - Here's the Kaggle that contains the data: https://www.kaggle.com/jbencina/department-of-justice-20092018-press-releases \n",
    "    - Here's the code the dataset creator used to scrape those press releases here if you're interested: https://github.com/jbencina/dojreleases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.0 Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helpful packages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "## nltk imports\n",
    "import nltk\n",
    "### uncomment and run these lines if you haven't downloaded relevant nltk add-ons yet\n",
    "### nltk.download('averaged_perceptron_tagger')\n",
    "### nltk.download('stopwords')\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "## spacy imports\n",
    "import spacy\n",
    "### uncomment and run the below line if you haven't loaded the en_core_web_sm library yet\n",
    "### ! python -m spacy download en_core_web_sm\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "## vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## sentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "## lda\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "\n",
    "## repeated printouts and wide-format text\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Load and clean text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File pset3_inputdata/combined.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## first, unzip the file pset3_inputdata.zip \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m## then, run this code to load the unzipped json file and convert to a dataframe\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m## (may need to change the pathname depending on where you store stuff)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m## and convert some of the attributes from lists to values\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m doj \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpset3_inputdata/combined.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m## due to json, topics are in a list so remove them and concatenate with ;\u001b[39;00m\n\u001b[1;32m      8\u001b[0m doj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopics_clean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(topic) \n\u001b[1;32m      9\u001b[0m                       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(topic) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo topic\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m     10\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m doj\u001b[38;5;241m.\u001b[39mtopics]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/json/_json.py:780\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    778\u001b[0m     convert_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 780\u001b[0m json_reader \u001b[38;5;241m=\u001b[39m JsonReader(\n\u001b[1;32m    781\u001b[0m     path_or_buf,\n\u001b[1;32m    782\u001b[0m     orient\u001b[38;5;241m=\u001b[39morient,\n\u001b[1;32m    783\u001b[0m     typ\u001b[38;5;241m=\u001b[39mtyp,\n\u001b[1;32m    784\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    785\u001b[0m     convert_axes\u001b[38;5;241m=\u001b[39mconvert_axes,\n\u001b[1;32m    786\u001b[0m     convert_dates\u001b[38;5;241m=\u001b[39mconvert_dates,\n\u001b[1;32m    787\u001b[0m     keep_default_dates\u001b[38;5;241m=\u001b[39mkeep_default_dates,\n\u001b[1;32m    788\u001b[0m     precise_float\u001b[38;5;241m=\u001b[39mprecise_float,\n\u001b[1;32m    789\u001b[0m     date_unit\u001b[38;5;241m=\u001b[39mdate_unit,\n\u001b[1;32m    790\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m    791\u001b[0m     lines\u001b[38;5;241m=\u001b[39mlines,\n\u001b[1;32m    792\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[1;32m    793\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m    794\u001b[0m     nrows\u001b[38;5;241m=\u001b[39mnrows,\n\u001b[1;32m    795\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    796\u001b[0m     encoding_errors\u001b[38;5;241m=\u001b[39mencoding_errors,\n\u001b[1;32m    797\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    798\u001b[0m     engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[1;32m    799\u001b[0m )\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/json/_json.py:893\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m filepath_or_buffer\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 893\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data_from_filepath(filepath_or_buffer)\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/json/_json.py:949\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    941\u001b[0m     filepath_or_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    943\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m filepath_or_buffer\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[1;32m    948\u001b[0m ):\n\u001b[0;32m--> 949\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_or_buffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    951\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    952\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal json to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    956\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    957\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File pset3_inputdata/combined.json does not exist"
     ]
    }
   ],
   "source": [
    "## first, unzip the file pset3_inputdata.zip \n",
    "## then, run this code to load the unzipped json file and convert to a dataframe\n",
    "## (may need to change the pathname depending on where you store stuff)\n",
    "## and convert some of the attributes from lists to values\n",
    "doj = pd.read_json(\"pset3_inputdata/combined.json\", lines = True)\n",
    "\n",
    "## due to json, topics are in a list so remove them and concatenate with ;\n",
    "doj['topics_clean'] = [\"; \".join(topic) \n",
    "                      if len(topic) > 0 else \"No topic\" \n",
    "                      for topic in doj.topics]\n",
    "\n",
    "## similarly with components\n",
    "doj['components_clean'] = [\"; \".join(comp) \n",
    "                           if len(comp) > 0 else \"No component\" \n",
    "                           for comp in doj.components]\n",
    "\n",
    "## drop older columns from data\n",
    "doj = doj[['id', 'title', 'contents', 'date', 'topics_clean', \n",
    "           'components_clean']].copy()\n",
    "\n",
    "doj.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tagging and sentiment scoring (17 points)\n",
    "\n",
    "Focus on the following press release: `id` == \"17-1204\" about this pharmaceutical kickback prosecution: https://www.forbes.com/sites/michelatindera/2017/11/16/fentanyl-billionaire-john-kapoor-to-plead-not-guilty-in-opioid-kickback-case/?sh=21b8574d6c6c \n",
    "\n",
    "The `contents` column is the one we're treating as a document. You may need to to convert it from a pandas series to a single string.\n",
    "\n",
    "We'll call the raw string of this press release `pharma`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code to subset to one press release and take the string\n",
    "pharma_doj = doj[doj['id'] == '17-1204']\n",
    "pharma = pharma_doj['contents'].iloc[0]\n",
    "\n",
    "# check to make sure we have raw string of press release\n",
    "print(type(pharma))\n",
    "pharma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 part of speech tagging (3 points)\n",
    "\n",
    "A. Preprocess the `pharma` press release to remove all punctuation / digits (you can use `.isalpha()` to subset)\n",
    "\n",
    "B. With the preprocessed press release from part A, use the part of speech tagger within nltk to tag all the words in that one press release with their part of speech. \n",
    "\n",
    "C. Using the output from B, extract the adjectives and sort those adjectives from most occurrences to fewest occurrences. Print a dataframe with the 5 most frequent adjectives and their counts in the `pharma` release. See here for a list of the names of adjectives within nltk: https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/\n",
    "\n",
    "**Resources**:\n",
    "\n",
    "- Documentation for `.isalpha()`: https://www.w3schools.com/python/ref_string_isalpha.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## restrict string to only words, removing punctuation and digits\n",
    "pharma_preprocessed = [word for word in word_tokenize(pharma.lower()) if word.isalpha()]\n",
    "pharma_preprocessed = ' '.join(pharma_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize words and then use part of speech tagger\n",
    "#pharma_preprocessed\n",
    "words = word_tokenize(pharma_preprocessed)\n",
    "#words\n",
    "tags = nltk.pos_tag(words)\n",
    "#tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get adjectives\n",
    "# got which tags to use from this website : https://cs.nyu.edu/~grishman/jet/guide/PennPOS.html\n",
    "adjectives = [word for word, tag in tags if tag in ['JJ', 'JJR', 'JJS', 'ADJ']]\n",
    "\n",
    "adj_counts = {}\n",
    "for adj in adjectives:\n",
    "    if adj in adj_counts:\n",
    "        adj_counts[adj] += 1\n",
    "    else:\n",
    "        adj_counts[adj] = 1\n",
    "print(adj_counts)\n",
    "sorted_adj_counts = sorted(adj_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "most_common_adjectives = sorted_adj_counts[:5]\n",
    "most_common_adjectives\n",
    "\n",
    "df_adjectives = pd.DataFrame(sorted_adj_counts, columns=['adjective', 'count'])\n",
    "print(df_adjectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 named entity recognition (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Using the original `pharma` press release (so the one before stripping punctuation/digits), use spaCy to extract all named entities from the press release.\n",
    "\n",
    "B. Print the unique named entities with the tag: `LAW`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use before preprocessed string to get named entity recognition\n",
    "spacy_pressrelease = nlp(pharma)\n",
    "print(type(spacy_pressrelease))\n",
    "# for one_tok in spacy_pressrelease.ents:\n",
    "#     print(\"Entity: \" + one_tok.text + \"; NER tag: \" + one_tok.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## unique entities w/ tag LAW\n",
    "law_tag = [one_tok.text for one_tok in spacy_pressrelease.ents if one_tok.label_ == 'LAW']\n",
    "law_tag = set(law_tag) \n",
    "law_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Use Google to summarize in one sentence what the `RICO` named entity means and why this might apply to a pharmaceutical kickbacks case (and not just a mafia case...) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RICO entity explanation\n",
    "\n",
    "Rico stands for the Racketeer Influenced and Corrupt Organizations Act (why it is given the tag 'LAW'), and it might apply to a pharmaceutical kickbacks case alongside mafia cases because RICO  and the definition of racketeering has expanded to include patterns of repeated crime - which could be a number of white-collar crimes like bribery, counterfeiting, theft, embezzlement, fraud, money laundering - in corporate settings which serves as the \"corrupt organization\", showing how it can apply to a pharmaceutical kickbacks case. \n",
    "\n",
    "\n",
    "source: https://en.wikipedia.org/wiki/Racketeer_Influenced_and_Corrupt_Organizations_Act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D. You want to extract the possible sentence lengths the CEO is facing; pull out the named entities with (1) the label `DATE` and (2) that contain the word year or years (hint: you may want to use the `re` module for that second part). Print these named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## named entities with DATE label and w/ year or years\n",
    "\n",
    "def extract_years(spacy_res):\n",
    "    years = []\n",
    "\n",
    "    # loop through named entities\n",
    "    for one_tok in spacy_pressrelease.ents:\n",
    "        if one_tok.label_ == 'DATE':\n",
    "            # check if 'year' or 'years' is in the named entity\n",
    "            if re.search(r'.*(year|years).*', one_tok.text):\n",
    "                years.append(one_tok.text)\n",
    "                \n",
    "    return years\n",
    "\n",
    "\n",
    "possible_sentences = extract_years(pharma)\n",
    "print(possible_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E. Pull and print the original parts of the press releases where those year lengths are mentioned (e.g., the sentences or rough region of the press release). Describe in your own words (1 sentence) what length of sentence (prison) and probation (supervised release) the CEO may be facing if convicted after this indictment (if there are multiple lengths mentioned describe the maximum). \n",
    "\n",
    "**Hint**: you may want to use re.search or re.findall \n",
    "\n",
    "- For part E, you can use `re.search` and `re.findall`, or anything that works 😳."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here\n",
    "year_pattern = r'([^.]*?years[^.]*\\.)'\n",
    "sentences_with_years = re.findall(year_pattern, pharma)\n",
    "for sentence in sentences_with_years:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum length of sentence for the CEO is 20 years, along with a 3-year probation period if convicted after the indictment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 sentiment analysis  (10 points)\n",
    "\n",
    "A. Subset the press releases to those labeled with one of three topics via `topics_clean`: Civil Rights, Hate Crimes, and Project Safe Childhood. We'll call this `doj_subset` going forward and it should have 717 rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here for subsetting\n",
    "topics = ['Civil Rights', 'Hate Crimes', 'Project Safe Childhood']\n",
    "doj_subset = doj[doj['topics_clean'].isin(topics)].copy().reset_index()\n",
    "doj_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Write a function that takes one press release string as an input and:\n",
    "\n",
    "- Removes named entities from each press release string (**Hint**: you may want to use `re.sub` with an or condition)\n",
    "- Scores the sentiment of the entire press release using the `SentimentIntensityAnalyzer` and `polarity_scores`\n",
    "- Returns the length-four (negative, positive, neutral, compound) sentiment dictionary (any order is fine)\n",
    "\n",
    "Apply that function to each of the press releases in `doj_subset`. \n",
    "\n",
    "**Hints**: \n",
    "\n",
    "- A function + list comprehension to execute will takes about 30 seconds on a respectable local machine and about 2 mins on jhub; if it's taking a very long time, you may want to check your code for inefficiencies. If you can't fix those, for partial credit on this part/full credit on remainder, you can take a small random sample of the 717\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here to define function\n",
    "def sentiment_analysis(press_string):\n",
    "    tokenized = word_tokenize(press_string)\n",
    "    tagged = pos_tag(tokenized)\n",
    "    \n",
    "    nlp_res = nlp(press_string)\n",
    "    without_ner = \" \".join([ent.text for ent in nlp_res if not ent.ent_type_])\n",
    "    \n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment = sia.polarity_scores(without_ner)\n",
    "    \n",
    "    return sentiment\n",
    "   \n",
    "# test  \n",
    "#print(sentiment_analysis('A federal jury convicted Rick Lee Evans, 43, of Anniston, Alabama, today of aggravated sexual abuse of a child after a five-day trial, Assistant Attorney General Leslie R. Caldwell of the Justice Department’s Criminal Division and U.S. Attorney Joyce White Vance of the Northern District of Alabama announced.  According to evidence introduced at trial, Evans, a former U.S. Army soldier, and his then-wife, a Department of Defense employee, were residing in Germany when they were asked to take temporary custody of a five-year-old child whose parents were deployed to Iraq with the U.S. Army.  Evans sexually abused the child on multiple occasions during the 18 months that the child lived with him from May 2007 to December 2008.  Trial Attorney Austin M. Berry of the Criminal Division’s Child Exploitation and Obscenity Section (CEOS) and Assistant U.S. Attorney Jacquelyn Hutzell of the Northern District of Alabama are prosecuting the case.  U.S. Army Criminal Investigations Division and the FBI’s Birmingham, Alabama, Division investigated the case. This case was brought as part of Project Safe Childhood, a nationwide initiative to combat the growing epidemic of child sexual exploitation and abuse, launched in May 2006 by the Department of Justice.  Led by U.S. Attorneys’ offices and CEOS, Project Safe Childhood marshals federal, state and local resources to better locate, apprehend and prosecute individuals who exploit children via the Internet, as well as to identify and rescue victims.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [sentiment_analysis(press_str) for press_str in doj_subset['contents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here executing the function\n",
    "doj_subset['sentiment_scores'] = doj_subset['contents'].apply(sentiment_analysis)\n",
    "doj_subset[['contents', 'sentiment_scores']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Add the four sentiment scores to the `doj_subset` dataframe to create a dataframe: `doj_subset_wscore`. Sort from highest neg to lowest neg score and print the top `id`, `contents`, and `neg` columns of the two most neg press releases. \n",
    "\n",
    "Notes:\n",
    "\n",
    "- Don't worry if your sentiment score differs slightly from our output on GitHub; differences in preprocessing can lead to diff scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## already added column to doj_subset df, so now create a copy\n",
    "doj_subset_wscore = doj_subset.copy().reset_index()\n",
    "doj_subset_wscore['neg_score'] = doj_subset_wscore['sentiment_scores'].apply(lambda x: x['neg'])\n",
    "doj_subset_wscore = doj_subset_wscore.sort_values('neg_score', ascending = False)\n",
    "\n",
    "# print id, contents, neg_score column for two most negative press releases\n",
    "for i in range(3):\n",
    "    print(doj_subset_wscore[['id', 'contents', 'neg_score']].iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D. With the dataframe from part C, find the mean compound sentiment score for each of the three topics in `topics_clean` using group_by and agg.\n",
    "\n",
    "E. Add a 1 sentence interpretation of why we might see the variation in scores (remember that compound is a standardized summary where -1 is most negative; +1 is most positive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## agg and find the mean compound score by topic\n",
    "doj_subset_wscore['compound_score'] = doj_subset_wscore['sentiment_scores'].apply(lambda x: x['compound'])\n",
    "compound_scores = doj_subset_wscore.groupby('topics_clean').agg({'compound_score': 'mean'}).reset_index()\n",
    "compound_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might see variation in compound scores due to the nature and emotional contexts of each type of crime. For example, hate crimes will have the most hateful, adverse language in their descriptions because of the nature of the crime, explaining the most negative compound score, whereas Civil Rights for example might not have outwardly hostile language, leading to a less negative compound score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Topic modeling (25 points)\n",
    "\n",
    "For this question, use the `doj_subset_wscores` data that is restricted to civil rights, hate crimes, and project safe childhood and with the sentiment scores added\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preprocess the data by removing stopwords, punctuation, and non-alpha words (5 points)\n",
    "\n",
    "A. Write a function that:\n",
    "\n",
    "- Takes in a single raw string in the `contents` column from that dataframe\n",
    "- Does the following preprocessing steps:\n",
    "\n",
    "    - Converts the words to lowercase\n",
    "    - Removes stopwords, adding the custom stopwords in your code cell below to the default stopwords list\n",
    "    - Only retains alpha words (so removes digits and punctuation)\n",
    "    - Only retains words 4 characters or longer\n",
    "    - Uses the snowball stemmer from nltk to stem\n",
    "\n",
    "- Returns a joined preprocessed string\n",
    "    \n",
    "B. Use `apply` or list comprehension to execute that function and create a new column in the data called `processed_text`\n",
    "    \n",
    "C. Print the `id`, `contents`, and `processed_text` columns for the following press releases:\n",
    "\n",
    "id = 16-718 (this case: https://www.seattletimes.com/nation-world/doj-miami-police-reach-settlement-in-civil-rights-case/)\n",
    "\n",
    "id = 16-217 (this case: https://www.wlbt.com/story/32275512/three-mississippi-correctional-officers-indicted-for-inmate-assault-and-cover-up/)\n",
    "    \n",
    "**Resources**:\n",
    "\n",
    "- Here's code examples for the snowball stemmer: https://www.geeksforgeeks.org/snowball-stemmer-nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_doj_stopwords = [\"civil\", \"rights\", \"division\", \"department\", \"justice\",\n",
    "                        \"office\", \"attorney\", \"district\", \"case\", \"investigation\", \"assistant\",\n",
    "                       \"trial\", \"assistance\", \"assist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code defining a text processing function\n",
    "def preprocess(words):\n",
    "    words = words.lower()\n",
    "    \n",
    "    stop_words = set(stopwords.words('english')).union(custom_doj_stopwords)\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    tokens = word_tokenize(words)\n",
    "    processed = [\n",
    "        stemmer.stem(tok) for tok in tokens\n",
    "        if tok.isalpha() and len(to tok not in stop_words andk) >= 4\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code executing the function\n",
    "doj_subset_wscore['processed_text'] = [preprocess(content) for content in doj_subset_wscore['contents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code showing the examples\n",
    "filtered_doj_wscores = doj_subset_wscore[doj_subset_wscore['id'].isin(['16-718', '16-217'])]\n",
    "filtered_doj_wscores[['id', 'contents', 'processed_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Create a document-term matrix from the preprocessed press releases and to explore top words (5 points)\n",
    "\n",
    "A. Use the `create_dtm` function I provide (alternately, feel free to write your own!) and create a document-term matrix using the preprocessed press releases; make sure metadata contains the following columns: `id`, `compound` sentiment column you added, and the `topics_clean` column\n",
    "\n",
    "B. Print the top 10 words for press releases with compound sentiment in the top 5% (so the most positive sentiment)\n",
    "\n",
    "C. Print the top 10 words for press releases with compound sentiment in the bottom 5% (so the most negative sentiment)\n",
    "\n",
    "**Hint**: for these, remember the pandas quantile function from pset one.  \n",
    "\n",
    "D. Print the top 10 words for press releases in each of the three `topics_clean`\n",
    "\n",
    "For steps B - D, to receive full credit, write a function `get_topwords` that helps you avoid duplicated code when you find top words for the different subsets of the data. There are different ways to structure it but one way is to feed it subsetted data (so data subsetted to one topic etc.) and for it to get the top words for that subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dtm(list_of_strings, metadata):\n",
    "    vectorizer = CountVectorizer(lowercase = True)\n",
    "    dtm_sparse = vectorizer.fit_transform(list_of_strings)\n",
    "    dtm_dense_named = pd.DataFrame(dtm_sparse.todense(), \n",
    "        columns=vectorizer.get_feature_names())\n",
    "    dtm_dense_named_withid = pd.concat([metadata.reset_index(), dtm_dense_named], axis = 1)\n",
    "    return(dtm_dense_named_withid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_doj = create_dtm(list_of_strings=doj_subset_wscore['processed_text'],\n",
    "                     metadata=doj_subset_wscore[['id', 'compound_score', 'topics_clean']])\n",
    "\n",
    "\n",
    "dtm_doj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_dtm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dtm \u001b[38;5;241m=\u001b[39m create_dtm(doj_subset_wscores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m], doj_subset_wscores[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompound\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopics_clean\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Step B: Print the top 10 words for press releases with compound sentiment in the top 5%\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_topwords\u001b[39m(data):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_dtm' is not defined"
     ]
    }
   ],
   "source": [
    "def get_topwords(dtm, num_words=10):\n",
    "    word_counts = dtm[dtm.columns[4:]].sum(axis=0).sort_values(ascending=False)\n",
    "    return word_counts.head(num_words)\n",
    "\n",
    "# thresholds for top and bottom 5% compound scores\n",
    "top_5_percent_threshold = dtm_doj['compound_score'].quantile(0.95)\n",
    "bottom_5_percent_threshold = dtm_doj['compound_score'].quantile(0.05)\n",
    "\n",
    "# get subsets based on thresholds\n",
    "top_5_percent_dtm = dtm_doj[dtm_doj['compound_score'] >= top_5_percent_threshold]\n",
    "bottom_5_percent_dtm = dtm_doj[dtm_doj['compound_score'] <= bottom_5_percent_threshold]\n",
    "\n",
    "print(\"top 10 words in the top 5% most positive:\")\n",
    "print(get_topwords(top_5_percent_dtm))\n",
    "print(\"top 10 words in the bottom 5% most negative:\")\n",
    "print(get_topwords(bottom_5_percent_dtm))\n",
    "\n",
    "# for topics\n",
    "for topic in dtm_doj['topics_clean'].unique():\n",
    "    topic_dtm = dtm_doj[dtm_doj['topics_clean'] == topic]\n",
    "    print(\"top 10 words for topic \" + topic + \":\")\n",
    "    print(get_topwords(topic_dtm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Estimate a topic model using those preprocessed words (5 points)\n",
    "\n",
    "A. Going back to the preprocessed words from part 2.3.1, estimate a topic model with 3 topics, since you want to see if the unsupervised topic models recover different themes for each of the three manually-labeled areas (civil rights; hate crimes; project safe childhood). You have free rein over the other topic model parameters beyond the number of topics.\n",
    "\n",
    "B. After estimating the topic model, print the top 15 words in each topic.\n",
    "\n",
    "**Hints and Resources**:\n",
    "\n",
    "- Same topic modeling resources linked to above\n",
    "- Make sure to use the `random_state` argument within the model so that the numbering of topics does not move around between runs of your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text\n",
    "doj_clean = doj_subset_wscore[doj_subset_wscore.processed_text != \"\"].copy()\n",
    "tokenized_text = [wordpunct_tokenize(one_text) \n",
    "                for one_text in \n",
    "                doj_clean.processed_text]\n",
    "#tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocess and estimate topicmod\n",
    "\n",
    "### create dictionary\n",
    "text_proc_dict = corpora.Dictionary(tokenized_text)\n",
    "\n",
    "### filter dictionary- using 2% as bounds\n",
    "text_proc_dict.filter_extremes(no_below = round(doj_clean.shape[0]*0.02),\n",
    "                             no_above = round(doj_clean.shape[0]*0.98))\n",
    "\n",
    "### create corpus from dictionary\n",
    "corpus_fromdict_proc = [text_proc_dict.doc2bow(one_text) \n",
    "                       for one_text in tokenized_text]\n",
    "\n",
    "# corpus_fromdict_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### estimate model\n",
    "n_topics = 3\n",
    "ldamod_proc = gensim.models.ldamodel.LdaModel(corpus_fromdict_proc, \n",
    "                                              num_topics = n_topics, \n",
    "                                              id2word=text_proc_dict, \n",
    "                                              passes=6, alpha = 'auto',\n",
    "                                              per_word_topics = True, \n",
    "                                              random_state = 91988)\n",
    "\n",
    "### print topics and words\n",
    "topics = ldamod_proc.print_topics(num_words = 15)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "lda_display_proc = gensimvis.prepare(ldamod_proc, corpus_fromdict_proc, text_proc_dict)\n",
    "pyLDAvis.display(lda_display_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Add topics back to main data and explore correlation between manual labels and our estimated topics (10 points)\n",
    "\n",
    "A. Extract the document-level topic probabilities. Within `get_document_topics`, use the argument `minimum_probability` = 0 to make sure all 3 topic probabilities are returned. Write an assert statement to make sure the length of the list is equal to the number of rows in the `doj_subset_wscores` dataframe\n",
    "\n",
    "B. Add the topic probabilities to the `doj_subset_wscores` dataframe as columns and create a column, `top_topic`, that reflects each document to its highest-probability topic (eg topic 1, 2, or 3)\n",
    "\n",
    "C. For each of the manual labels in `topics_clean` (Hate Crime, Civil Rights, Project Safe Childhood), print the breakdown of the % of documents with each top topic (so, for instance, Hate Crime has 246 documents-- if 123 of those documents are coded to topic_1, that would be 50%; and so on). **Hint**: pd.crosstab and normalize may be helpful: https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.crosstab.html\n",
    "\n",
    "D. Using a couple press releases as examples, write a 1-2 sentence interpretation of why some of the manual topics map on more cleanly to an estimated topic than other manual topic(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here to get doc-level topic probabilities \n",
    "document_topics = [ldamod_proc.get_document_topics(item, minimum_probability=0) for item in corpus_fromdict_proc]\n",
    "assert len(document_topics) == len(doj_subset_wscore), \"the length of the list is not equal to the number of rows in the doj_subset_wscores dataframe\"\n",
    "print(len(document_topics))\n",
    "print(len(doj_subset_wscore))\n",
    "#length of both is 717"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "doj_subset_wscore['topic_1'] = [\n",
    "    next((t[1] for t in doc if t[0] == 0))  # Extract the probability for topic 1, or 0 if not found\n",
    "    for doc in document_topics\n",
    "]\n",
    "doj_subset_wscore['topic_2'] = [\n",
    "    next((t[1] for t in doc if t[0] == 1))  # Extract the probability for topic 1, or 0 if not found\n",
    "    for doc in document_topics\n",
    "]\n",
    "doj_subset_wscore['topic_3'] = [\n",
    "    next((t[1] for t in doc if t[0] == 2))  # Extract the probability for topic 1, or 0 if not found\n",
    "    for doc in document_topics\n",
    "]\n",
    "doj_subset_wscore['top_topic'] = doj_subset_wscore[['topic_1', 'topic_2', 'topic_3']].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## topic proportions\n",
    "topic_distribution = pd.crosstab(doj_subset_wscore['topics_clean'], doj_subset_wscore['top_topic'], normalize='index') * 100\n",
    "print(topic_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_docs = doj_subset_wscore.sample(n=3, random_state=56)  \n",
    "\n",
    "for index, row in random_docs.iterrows():\n",
    "    print(\"document ID:\", row['id'])\n",
    "    print(\"contents:\", row['contents'])\n",
    "    print(\"actual topic:\", row['topics_clean'])\n",
    "    print(\"predicted topic (LDA):\", row['top_topic'])\n",
    "    print(\"-------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main reason that some manual topics map on more cleanly to an estimated topic than other manual topic is due to the consistency or inconsistency of language used when describing the topic. For example, 'hate crime' documents predominantly map to topic_1, which may have more consistent themes and languages associated with that type of crime, like 'discrimination' or 'violence' or frequent types of hate crimes like graffiti or slurs, which helps our LDA model effectively categorize these documents. On the other hand, 'civil rights' documents can sometimes be misclassified and less consistently mapped, which is likely due to the more diverse content and issues that might lie under the category of 'civil rights'. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extend the analysis from unigrams to bigrams (10 points)\n",
    "\n",
    "In the previous question, you found top words via a unigram representation of the text. Now, we want to see how those top words change with bigrams (pairs of words)\n",
    "\n",
    "A. Using the `doj_subset_wscore` data and the `processed_text` column (so the words after stemming/other preprocessing), create a column in the data called `processed_text_bigrams` that combines each consecutive pairs of word into a bigram separated by an underscore. Eg:\n",
    "\n",
    "\"depart reach settlem\" would become \"depart_reach reach_settlem\"\n",
    "\n",
    "Do this by writing a function `create_bigram_onedoc` that takes in a single `processed_text` string and returns a string with its bigrams structured similarly to above example\n",
    " \n",
    "**Hint**: there are many ways to solve but `zip` may be helpful: https://stackoverflow.com/questions/21303224/iterate-over-all-pairs-of-consecutive-items-in-a-list\n",
    "\n",
    "B. Print the `id`, `processed_text`, and `processed_text_bigram` columns for press release with id = 16-217"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A\n",
    "def create_bigram_onedoc(text):\n",
    "    words = text.split()\n",
    "    bigrams = ['_'.join(bigram) for bigram in zip(words, words[1:])]\n",
    "    return ' '.join(bigrams)\n",
    "\n",
    "\n",
    "doj_subset_wscore['processed_text_bigrams'] = [create_bigram_onedoc(processed_content) for processed_content in doj_subset_wscore['processed_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## B\n",
    "filtered_doj_wscores = doj_subset_wscore[doj_subset_wscore['id'].isin(['16-217'])]\n",
    "filtered_doj_wscores[['id', 'processed_text', 'processed_text_bigrams']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Use the create_dtm function and the `processed_text_bigrams` column to create a document-term matrix (`dtm_bigram`) with these bigrams. Keep the following three columns in the data: `id`, `topics_clean`, and `compound` \n",
    "\n",
    "D. Print the (1) dimensions of the `dtm` matrix from question 2.2  and (2) the dimensions of the `dtm_bigram` matrix. Comment on why the bigram matrix has more dimensions than the unigram matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_bigram = create_dtm(list_of_strings=doj_subset_wscore['processed_text_bigrams'],\n",
    "                     metadata=doj_subset_wscore[['id', 'compound_score', 'topics_clean']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of initial dtm matrix: \" + str(dtm_doj.shape))\n",
    "print(\"shape of bigram dtm matrix: \" + str(dtm_bigram.shape))\n",
    "\n",
    "# COMMENT ON WHY BIGRAM MATRIX HAS MORE DIMENSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E. Find and print the 10 most prevelant bigrams for each of the three topics_clean using the `get_topwords` function from 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "for topic in dtm_bigram['topics_clean'].unique():\n",
    "    topic_dtm = dtm_bigram[dtm_bigram['topics_clean'] == topic]\n",
    "    print(\"top 10 words for topic \" + topic + \":\")\n",
    "    print(get_topwords(topic_dtm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optional extra credit (2 points)\n",
    "\n",
    "You notice that the pharmaceutical kickbacks press release we analyzed in question 1 was for an indictment, and that in the original data, there's not a clear label for whether a press release outlines an indictment (charging someone with a crime), a conviction (convicting them after that charge either via a settlement or trial), or a sentencing (how many years of prison or supervised release a defendant is sentenced to after their conviction).\n",
    "\n",
    "You want to see if you can identify pairs of press releases where one press release is from one stage (e.g., indictment) and another is from a different stage (e.g., a sentencing).\n",
    "\n",
    "You decide that one way to approach is to find the pairwise string similarity between each of the processed press releases in `doj_subset`. There are many ways to do this, so Google for some approaches, focusing on ones that work well for entire documents rather than small strings.\n",
    "\n",
    "Find the top two pairs (so four press releases total)-- do they seem like different stages of the same crime or just press releases covering similar crimes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
